-----------------------------hadoop安装----------------------------------------
1、基础配置(4个节点)
主机列表：
============================
master  192.168.137.69
slave1  192.168.137.70
slave2  192.168.137.71
slave3  192.168.137.72
============================
1)、ssh免密
2)、/etc/security/limits.conf
============================
*        -       nproc          102480  
*        -       nofile         102480
============================
3)、/etc/hostname
4)、/etc/hosts
5)、安装jdk1.8
6)、关闭防火墙
systemctl stop firewalld
systemctl status firewalld
7)、校准时间
ntpdate cn.pool.ntp.org #通过服务器校准时间
date #查看时间
8)、文件夹建立
cd /home/hadoop/
mkdir -m 755 datadir
mkdir -m 755 tmp

2、解压缩hadoop安装包并配置环境变量
mv hadoop-3.1.3.tar.gz /opt/
tar -xzvf hadoop-3.1.3.tar.gz
mv hadoop-3.1.3 hadoop
export HADOOP_HOME=/opt/hadoop
export PATH=$PATH:$HADOOP_HOME/sbin
export PATH=$PATH:$HADOOP_HOME/bin

3、配置配置文件
vim core-site.xml
=====================================
<property>
   <name>fs.defaultFS</name>
   <value>hdfs://master:9000</value>
</property>
=====================================

vim hadoop-env.sh
=====================================
export JAVA_HOME=/opt/jdk8
=====================================

vim hdfs-site.xml
=====================================
<property>
  <name>dfs.namenode.name.dir</name>
  <value>/opt/hadoop/namedir</value>
</property>
<property>
   <name>dfs.datanode.data.dir</name>
   <value>/opt/hadoop/datadir</value>
</property>
<property>
   <name>hadoop.tmp.dir</name>
   <value>/opt/hadoop/tmp</value>
</property>
<property>
   <name>dfs.replication</name>
   <value>2</value>
</property>
<property>
  <name>dfs.permissions</name>
  <value>false</value>
</property>
<property>
  <name>dfs.webhdfs.enabled</name>
  <value>true</value>
</property>
<property>
  <name>dfs.support.append</name>
  <value>true</value>
</property>
<property>
  <name>hadoop.proxyuser.hadoop.hosts</name>
  <value>*</value>
</property> 
<property>
  <name>hadoop.proxyuser.hadoop.groups</name>
  <value>*</value>
</property>
<property>
  <name>dfs.ha.fencing.methods</name>
  <value>sshfence</value>
</property>
<property>
  <name>dfs.ha.fencing.ssh.private-key-files</name>
  <value>/home/hadoop/.ssh/id_rsa</value>
</property>
=====================================

cp mapred-site.xml.template mapred-site.xml
vim mapred-site.xml
=====================================
<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>
=====================================

vim yarn-site.xml
=====================================
<property>
  <name>yarn.resourcemanager.webapp.address</name>
  <value>master:8088</value>
</property>
<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>master</value>
</property>
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>
<property>
  <name>yarn.application.classpath</name>
  <value>/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*</value>  #通过hadoop classpath获取value内容
</property>

=====================================

vim workers
=====================================
slave1
slave2
slave3
=====================================

4、格式化
hdfs namenode -format

5、启动
start-dfs.sh
start-yarn.sh

6、测试
hdfs dfsadmin -report
hdfs:   http://master:9870 
yarn:   http://master:8088


------------------------------hbase安装----------------------------------------
1、解压缩hbase安装包并配置环境变量
mv hbase-2.2.2-bin.tar.gz /opt/
tar -xzvf hbase-2.2.2-bin.tar.gz
mv hbase-2.2.2-bin.tar.gz hbase
export HBASE_HOME=/opt/hbase
export PATH=$PATH:$HBASE_HOME/bin

2、配置配置文件
vim hbase-env.sh
=====================================
export JAVA_HOME=/opt/jdk8/
export HBASE_MANAGES_ZK=false
export HBASE_DISABLE_HADOOP_CLASSPATH_LOOKUP="true"
=====================================

vim hbase-site.xml
=====================================
    <property>
        <name>hbase.master</name>
        <value>master:16000</value>
    </property>
    <property>
        <name>hbase.master.maxclockskew</name>
        <value>180000</value>
    </property>
    <property>
        <name>hbase.rootdir</name>
        <value>hdfs://master:9000/hbase</value>
    </property>
    <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
    </property>
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>slave1,slave2,slave3</value>
    </property>
    <property>
       <name>hbase.unsafe.stream.capability.enforce</name>
       <value>false</value>
   </property>
=====================================

vim regionservers
=====================================
slave1
slave2
slave3
=====================================

3、启动
先启动zookeeper
start-hbase.sh

4、测试

web http://master:16010/

5、命令测试
hbase shell
list
create 'mytb','cf1','cf2'

-------------------------hive安装(仅master安装)---------------------------------
1、安装msyql
sudo apt install mysql*
su - root
修改配置文件/etc/mysql/mysql.conf.d/mysqld.cnf
=====================================
bind-address            = 0.0.0.0
=====================================
systemctl restart mysql
mysql(先进入数据库)
>USE mysql;
>update user set authentication_string=password('root') where user='root' and Host = 'localhost';
>UPDATE user SET plugin='mysql_native_password' WHERE User='root';(设置必须使用密码才能登录)
>GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;
>FLUSH PRIVILEGES;
>exit
mysql -uroot -proot
>create database hive;

2、解压缩hive安装包并配置环境变量
tar -zxvf apache-hive-3.1.2-bin.tar.gz
mv apache-hive-3.1.2 hive
export HIVE_HOME=/opt/hive
export PATH=$PATH:$HIVE_HOME/bin
将mysql-connector-java-5.1.48-bin.jar放到hive的lib文件夹中，用bin.jar和jar均可
rm /opt/hive/lib/guava-19.0.jar
cp /opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar /opt/hive/lib/

cd /opt/hive
mkdir tmpdir
chmod 755 tmpdir

3、配置配置文件
cp hive-env.sh.template hive-env.sh
vim hive-env.sh
=====================================
HADOOP_HOME=/opt/hadoop
export HIVE_CONF_DIR=/opt/hive/conf
export HIVE_AUX_JARS_PATH=/opt/hive/lib
=====================================

cp hive-default.xml.template hive-site.xml
vim hive-site.xml
=====================================
        <property>
                <name>javax.jdo.option.ConnectionURL</name>
                <value>jdbc:mysql://192.168.137.69:3306/hive</value>
        </property>

        <property>
                <name>javax.jdo.option.ConnectionDriverName</name>
                <value>com.mysql.jdbc.Driver</value>
        </property>

        <property>
                <name>javax.jdo.option.ConnectionUserName</name>
                <value>root</value>
        </property>

        <property>
                <name>javax.jdo.option.ConnectionPassword</name>
                <value>root</value>
        </property>
        <property>
                <name>hive.server2.thrift.client.user</name>
                <value>test</value>
                <description>Username to use against thrift client</description>
         </property>
         <property>
                <name>hive.server2.thrift.client.password</name>
                <value>test</value>
                <description>Password to use against thrift client</description>
         </property>
         <property>
                <name>hive.exec.local.scratchdir</name>
                <value>/opt/hive/tmpdir/${user.name}</value>
                <description>Local scratch space for Hive jobs</description>
         </property>


并将3215行第96列的那几个特殊字符删除，否则报错！
将内容中出现${system:java.io.tmpdir}的替换成/opt/hive/tmpdir
=====================================

mv hive-log4j2.properties.template hive-log4j2.properties
vim hive-log4j2.properties
=====================================
property.hive.log.dir = /opt/hive/tmpdir
=====================================

4、初始化数据库
schematool -dbType mysql -initSchema

5、测试
hive
>show databases;

hive --service hiveserver2
beeline -u jdbc:hive2://localhost:10000 -n test -p test

-------------------------flink安装(standalone)---------------------------------
1、解压缩hive安装包并配置环境变量
tar -zxvf flink-1.12.0-bin-scala_2.12.tgz
mv flink-1.12.0-bin-scala_2.12.tgz flink
export FLINK_HOME=/opt/flink
export PATH=$PATH:$FLINK_HOME/bin

2、配置配置文件
vim flink-conf.yaml
=====================================
jobmanager.rpc.address: master
=====================================

vim masters
=====================================
master:8081
=====================================

vim workers
=====================================
slave1
slave2
slave3
=====================================

3、启动
start-cluster.sh

4、测试
web http://master:8081

提交任务命令：flink run -c org.apache.flink.examples.java.wordcount.WordCount -p 2 ./WordCount.jar --host master --port 8081

取消任务：
flink list #查看任务列表以及任务id
flink cancel id #取消任务
flink list -a #查看被取消的任务

-------------------------flink安装(HA)-----------------------------------------
1、修改配置文件
vim flink-conf.yaml
=====================================
jobmanager.rpc.address: master #slave1节点修改为：jobmanager.rpc.address: slave1
high-availability: zookeeper
high-availability.zookeeper.quorum: slave1:2181,slave2:2181,slave3:2181
high-availability.zookeeper.storageDir: hdfs://master:9000/flink/ha
=====================================

vim masters
=====================================
master:8081
slave1:8081
=====================================

vim zoo.cfg
=====================================
server.1=slave1:2888:3888
server.2=slave2:2888:3888
server.3=slave3:2888:3888
=====================================

2、添加环境变量
export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
将commons-cli-1.4.jar、flink-shaded-hadoop-3-uber-3.1.1.7.1.1.0-565-9.0.jar和flink-hadoop-fs-1.12.0.jar放入flink的lib目录下

2、启动hdfs
3、启动zookeeper
4、启动flink集群
start-cluster.sh 

5、测试
web http://master:8081
    http://slave1:8081


-------------------------kafka安装---------------------------------------------
1、解压缩kafka安装包并配置环境变量
tar xzf  kafka_2.13-3.0.0.tgz
mv kafka_2.13-3.0.0 kafka
export KAFKA_HOME=/opt/kafka
export PATH=$PATH:$KAFKA_HOME/bin

2、配置配置文件
cd /opt/kafka/config/kraft
vim server.properties
=====================================
node.id=1    #三个节点分别是1、2、3
controller.quorum.voters=1@slave1:9093,2@slave2:9093,3@slave3:9093
listeners=PLAINTEXT://slave1:9092,CONTROLLER://slave1:9093 #三个节点分别是slave1、slave2、slave3
advertised.listeners=PLAINTEXT://slave1:9092 #三个节点分别是slave1、slave2、slave3

=====================================

3、配置集群id
kafka-storage.sh random-uuid
l1uUYAZ3Q8uSYE1T8KbMMw

kafka-storage.sh format -t l1uUYAZ3Q8uSYE1T8KbMMw -c /opt/kafka/config/kraft/server.properties #三个节点执行

4、启动
kafka-server-start.sh -daemon /opt/kafka/config/kraft/server.properties

5、测试
创建topic:
kafka-topics.sh --create --bootstrap-server slave1:9092,slave2:9092,slave3:9092 --topic topicName --partitions 3 --replication-factor 2

列出topic：
kafka-topics.sh --list --bootstrap-server slave1:9092,slave2:9092,slave3:9092 

-------------------------spark安装---------------------------------------------
spark standalone部署
1、解压缩scala-2.11.12.zip，并且添加环境变量SCALA_HOME、PATH
以及添加以下环境变量：
export TERM=xterm-color

2、解压缩spark-2.4.5-bin-hadoop2.7.tgz（只能下载这个2.7，没有hadoop3.1的对应版）,并且添加环境变量SPARK_HOME、PATH,注意path只配置bin目录即可

3、将spark/conf/spark-env.sh.template复制为spark-env.sh,并修改spark-env.sh中的JAVA_HOME、SCALA_HOME：

export JAVA_HOME=/opt/jdk8
export SCALA_HOME=/opt/scala-2.11.12
export SPARK_MASTER_IP=master
export SPARK_MASTER_PORT=7077
export SPARK_WORKER_CORES=1
export SPARK_WORKER_INSTANCES=1
export SPARK_WORKER_MEMORY=1g

修改spark/conf/slaves:
去掉localhost，添加Spark worker的hostname, 一行一个如下：
slave1
slave2
slave3

4、将配置好的spark分发到slave机器上
scp -r /opt/spark hadoop@slave1:/opt
scp -r /opt/spark hadoop@slave2:/opt
scp -r /opt/spark hadoop@slave3:/opt
5、启动hadoop
start-dfs.sh

6.启动spark
/opt/spark/sbin/start-all.sh

7.检查是否安装成功：
master上jps：
显示Master进程

slave上jps：
显示Worker进程

浏览master的web UI：http://master:8080

8、运行spark-shell
在根目录下：/opt/spark/bin/spark-shell
scala> val data = sc.textFile("hdfs://master:9000/README.txt")
scala> val res=data.flatMap(_.split("")).map((_,1)).reduceByKey(_+_).map(x=>(x._2,x._1)).sortByKey().map(x=>(x._2,x._1))

scala> res.collect;
scala> res.saveAsTextFile("hdfs://master:9000/out")

9、提交代码测试：
cd /opt/spark/examples/jars

spark-submit --class org.apache.spark.examples.SparkPi --master spark://master:7077 --executor-memory 512m --total-executor-cores 1 spark-examples_2.11-2.4.5.jar 100

=====================================
spark standalone解决单点的HA部署

1、启动zookeeper
2、修改spark-env.sh：
首先注释掉单节点主的配置
#export SPARK_MASTER_IP=master
#export SPARK_MASTER_PORT=7077
然后加入
export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=slave1:2181,slave2:2181,slave3:2181 -Dspark.deploy.zookeeper.dir=/spark"

【spark.deploy.recoveryMode 设置成ZOOKEEPER】
【park.deploy.zookeeper.url ZooKeeper URL】
【spark.deploy.zookeeper.dir ZooKeeper保存恢复状态的目录，缺省为zookeeper里面的/spark】
3、启动spark，并且在找台机器启动一个主，则两个主互为failover
/opt/spark/sbin/start-master.sh

浏览slave1的web UI：http://slave1:8082

4、MASTER=spark://master:7077,slave1:7077 /opt/spark/bin/spark-shell

=====================================
spark on yarn部署
1、修改spark-env.sh,加入hadoop路径
export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop

2、往yarn提交任务需要增加两个配置yarn-site.xml
<property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
</property>

<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>

scp -r /opt/hadoop/etc/hadoop/yarn-site.xml hadoop@slave1:/opt/hadoop/etc/hadoop
scp -r /opt/hadoop/etc/hadoop/yarn-site.xml hadoop@slave2:/opt/hadoop/etc/hadoop
scp -r /opt/hadoop/etc/hadoop/yarn-site.xml hadoop@slave3:/opt/hadoop/etc/hadoop

3、启动yarn(不用启动spark了，spark只是客户端了)
start-yarn.sh


4、检查
cd /opt/spark/examples/jars


spark on yarn client模式 日志在本地输出，一班用于上线前测试

spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --executor-memory 512M --num-executors 2 spark-examples_2.11-2.4.5.jar 100


spark on yarn cluster模式 上线使用，不会再本地打印日志,减少io

spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster --executor-memory 512m --num-executors 2 --executor-cores 1 spark-examples_2.11-2.4.5.jar 100


获取yarn程序执行日志,执行成功之后才能获取到

yarn logs -applicationId application_1669452462734_0002

#yarn ui
http://master:8088


=====================================
hive on spark部署
1、首先在slave1上解压缩一个hive，为了不影响原始的mr hive（此处可以从master上copy过来一个修改，务必删除tempdir文件夹）
2、启动hdfs、yarn和spark(standalone即可)

3、在hive中创建spark配置文件

vim /opt/hive/conf/spark-defaults.conf
添加如下内容

spark.master                             yarn
spark.eventLog.enabled                   true
spark.eventLog.dir                       hdfs://master:9000/spark-history 
spark.executor.memory                    1g
spark.driver.memory                      1g


在HDFS创建如下路径，用于存储历史日志
hadoop fs -mkdir /spark-history

4、向HDFS上传Spark纯净版jar包

上传并解压spark-2.4.5-bin-without-hadoop.tgz
tar -zxvf /opt/spark-2.4.5-bin-without-hadoop.tgz
上传Spark纯净版jar包到HDFS
hadoop fs -mkdir /spark-jars
hadoop fs -put spark-2.4.5-bin-without-hadoop/jars/* /spark-jars


5、修改hive-site.xml文件
vi /opt/hive/conf/hive-site.xml
 
添加如下内容
 
<!--Spark依赖位置-->
 
<property>
    <name>spark.yarn.jars</name>
    <value>hdfs://master:9000/spark-jars/*</value>
</property>
<!--Hive执行引擎-->
<property>
    <name>hive.execution.engine</name>
    <value>spark</value>
</property>
<!--hive和spark连接超时问题-->
<property>
    <name>hive.spark.client.connect.timeout</name>
    <value>100000ms</value>
</property>
<property>
    <name>hive.metastore.schema.verification</name>
    <value>false</value>
</property>

6、修改spark-env.sh 文件(内容通过hadoop classpath获取)


export SPARK_DIST_CLASSPATH=/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/*:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/*:/opt/hadoop/share/hadoop/hdfs/*:/opt/hadoop/share/hadoop/mapreduce/lib/*:/opt/hadoop/share/hadoop/mapreduce/*:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/*:/opt/hadoop/share/hadoop/yarn/*
export YARN_CONF_DIR=/opt/hadoop/etc/hadoop
export SPARK_CLASSPATH=${SPARK_CLASSPATH}:/opt/hive/lib/mysql-connector-java-5.1.48-bin.jar

7、初始化数据库
schematool -dbType mysql -initSchema

8、将spark和scala的三个jar拷贝到hive的lib下
cp /opt/spark/jars/spark-core_2.11-2.4.5.jar /opt/hive/lib/
cp /opt/scala-2.11.12/lib/scala-library.jar /opt/hive/lib/
cp /opt/spark/jars/spark-network-common_2.11-2.4.5.jar /opt/hive/lib/
cp /opt/spark/jars/spark-unsafe_2.11-2.4.5.jar /opt/hive/lib/


9、测试

create database testdb;
use testdb;
create table testtb(id int,name string)ROW FORMAT DELIMITED FIELDS TERMINATED BY ",";

select count(*) from testtb where name='aaa';






